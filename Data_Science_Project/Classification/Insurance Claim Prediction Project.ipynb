{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51297df5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this project is to classify whether an insurance claim will be made based on various features of the insured individual. We will explore and implement three machine learning models: Logistic Regression, Random Forest, and Support Vector Machine (SVM). The dataset used for this project contains information such as age, BMI, steps, number of children, sex, smoking status, and region.\n",
    "\n",
    "## Data Preprocessing\n",
    "We start by loading and preprocessing the dataset. This involves converting categorical variables to dummy variables, handling missing values, and splitting the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('claims_data.csv')\n",
    "\n",
    "# Convert target variable to binary (1 if there was a claim, 0 otherwise)\n",
    "df['insurance_claim'] = df['insurance_claim'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Create dummy variables for categorical features and drop the first in each instance\n",
    "df_dummies = pd.get_dummies(df.drop(columns=['claim_amount']), drop_first=True)\n",
    "\n",
    "# Define features and target variable\n",
    "X = df_dummies.drop(columns=['insurance_claim'])\n",
    "y = df_dummies['insurance_claim']\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "bool_columns = X.select_dtypes(include=['bool']).columns\n",
    "X[bool_columns] = X[bool_columns].astype(int)\n",
    "\n",
    "# Ensure all data is numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "# Drop any rows with NaN values that could not be converted\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]  # Ensure y has the same indices as X after dropping NaNs\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dc7cf",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Next, we fit a logistic regression model to the data and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the X matrices\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.Logit(y_train, X_train)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(result.summary())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = result.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Proportion of correctly predicted claim indicators: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f9ba0",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "We then fit a Random Forest model to the data and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be326edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and fit the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate false positives and false negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d1d80",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "Finally, we fit SVM models with different kernels and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and fit the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate false positives and false negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d359610",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, we implemented and compared three classification models to predict insurance claims. The Random Forest model performed the best, achieving the highest accuracy and the lowest number of false positives and false negatives. Further tuning and model optimization can be explored to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
