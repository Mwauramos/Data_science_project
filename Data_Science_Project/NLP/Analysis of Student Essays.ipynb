{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77b5cd5",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Analysis of Student Essays\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project focuses on analyzing a dataset of student essays using various Natural Language Processing (NLP) techniques. We'll explore different aspects of the essays, including word frequencies, personality types, and bi-gram analysis. The dataset contains essays written by students along with their Myers-Briggs Type Indicator (MBTI) personality types.\n",
    "\n",
    "Our main objectives are to:\n",
    "1. Preprocess the text data\n",
    "2. Analyze word frequencies and unique words\n",
    "3. Explore the relationship between personality types and language use\n",
    "4. Create and analyze bi-grams\n",
    "\n",
    "Let's begin by importing the necessary libraries and loading our data.\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, we'll import the required libraries and load our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d4cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93 entries, 0 to 92\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   I/E     93 non-null     object\n",
      " 1   N/S     93 non-null     object\n",
      " 2   T/F     93 non-null     object\n",
      " 3   J/P     93 non-null     object\n",
      " 4   Essay   93 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.8+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "  I/E N/S T/F J/P                                              Essay\n",
      "0   I   S   T   J  My first 4 months at the EDSA have been filled...\n",
      "1   I   N   F   J  I joined the academy being at a crossroads of ...\n",
      "2   E   N   F   J  so far my experience has been positive and i c...\n",
      "3   I   N   F   J  I have been very fortunate to have the opportu...\n",
      "4   I   N   T   J  Looking back to when one got to the academy an...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Amoh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amoh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams, bigrams\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Essay_data.csv')\n",
    "\n",
    "# Remove rows with missing values and reset index\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Display basic information about the dataframe\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad70fd6",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Functions\n",
    "Next, we'll define some helper functions for text preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffc05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove punctuation and convert to lower case\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff6867",
   "metadata": {},
   "source": [
    "## 3. Analyzing the First Essay\n",
    "Let's start by analyzing the first essay in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e9032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10th character in the first essay is: 4\n",
      "Number of tokens in the first essay: 275\n"
     ]
    }
   ],
   "source": [
    "# Get the first essay\n",
    "first_essay = df['Essay'][0]\n",
    "\n",
    "# Preprocess and tokenize\n",
    "preprocessed_essay = preprocess_text(first_essay)\n",
    "tokens = tokenize_and_remove_stopwords(preprocessed_essay)\n",
    "\n",
    "# Find the 10th character\n",
    "tenth_character = preprocessed_essay[9]\n",
    "\n",
    "print(f\"The 10th character in the first essay is: {tenth_character}\")\n",
    "print(f\"Number of tokens in the first essay: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45be7e",
   "metadata": {},
   "source": [
    "## 4. Analyzing Word Frequencies\n",
    "Now, let's examine word frequencies across all essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886c23c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in all essays (after removing stopwords) is: 3406\n",
      "Percentage of words that appear at least twice: 89.88%\n"
     ]
    }
   ],
   "source": [
    "# Process all essays\n",
    "all_words = []\n",
    "\n",
    "for essay in df['Essay']:\n",
    "    preprocessed_essay = preprocess_text(essay)\n",
    "    tokens = tokenize_and_remove_stopwords(preprocessed_essay)\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "# Create a bag of words for all essays\n",
    "bag_of_words = Counter(all_words)\n",
    "\n",
    "# Count unique words\n",
    "unique_word_count = len(set(all_words))\n",
    "\n",
    "print(f\"The number of unique words in all essays (after removing stopwords) is: {unique_word_count}\")\n",
    "\n",
    "# Calculate percentage of words appearing at least twice\n",
    "words_at_least_twice = sum(count for word, count in bag_of_words.items() if count >= 2)\n",
    "total_words = sum(bag_of_words.values())\n",
    "percentage = (words_at_least_twice / total_words) * 100\n",
    "\n",
    "print(f\"Percentage of words that appear at least twice: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691bcfde",
   "metadata": {},
   "source": [
    "## 5. Personality Type Analysis\n",
    "Let's analyze word usage by specific personality types, focusing on ENFJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f4edc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ENFJ essays: 6\n",
      "The most commonly mentioned word by ENFJ personalities is 'team' with 33 occurrences.\n",
      "\n",
      "Top 10 most common words for ENFJ:\n",
      "team: 33\n",
      "work: 23\n",
      "working: 14\n",
      "different: 13\n",
      "people: 12\n",
      "like: 12\n",
      "making: 12\n",
      "academy: 12\n",
      "new: 12\n",
      "great: 11\n"
     ]
    }
   ],
   "source": [
    "# Filter for ENFJ personalities\n",
    "enfj_df = df[(df['I/E'] == 'E') & (df['N/S'] == 'N') & (df['T/F'] == 'F') & (df['J/P'] == 'J')]\n",
    "\n",
    "# Process ENFJ essays\n",
    "enfj_words = []\n",
    "\n",
    "for essay in enfj_df['Essay']:\n",
    "    preprocessed_essay = preprocess_text(essay)\n",
    "    tokens = tokenize_and_remove_stopwords(preprocessed_essay)\n",
    "    enfj_words.extend(tokens)\n",
    "\n",
    "# Create a bag of words for ENFJ essays\n",
    "enfj_bag_of_words = Counter(enfj_words)\n",
    "\n",
    "# Find the most common word\n",
    "most_common_word = enfj_bag_of_words.most_common(1)[0]\n",
    "\n",
    "print(f\"Number of ENFJ essays: {len(enfj_df)}\")\n",
    "print(f\"The most commonly mentioned word by ENFJ personalities is '{most_common_word[0]}' with {most_common_word[1]} occurrences.\")\n",
    "\n",
    "print(\"\\nTop 10 most common words for ENFJ:\")\n",
    "for word, count in enfj_bag_of_words.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7607a",
   "metadata": {},
   "source": [
    "## 6. Bi-gram Analysis\n",
    "Finally, let's create bi-grams for each essay and analyze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96718f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 109th bi-gram in the 70th essay is: ('work', 'quite')\n",
      "\n",
      "First few bi-grams of the 70th essay:\n",
      "[('academy', 'taught'), ('taught', 'lot'), ('lot', 'honestly'), ('honestly', 'starting'), ('starting', 'class'), ('class', 'hundred'), ('hundred', 'people'), ('people', 'experience'), ('experience', 'knowledge'), ('knowledge', 'wisdom')]\n"
     ]
    }
   ],
   "source": [
    "def get_bigrams(tokens):\n",
    "    return list(bigrams(tokens))\n",
    "\n",
    "# Create a new column with bi-grams\n",
    "df['bigrams'] = df['Essay'].apply(lambda x: get_bigrams(tokenize_and_remove_stopwords(preprocess_text(x))))\n",
    "\n",
    "# Get the 70th essay (index 69 since Python uses 0-based indexing)\n",
    "seventieth_essay_bigrams = df.loc[69, 'bigrams']\n",
    "\n",
    "# Check if there are at least 109 bi-grams\n",
    "if len(seventieth_essay_bigrams) >= 109:\n",
    "    print(f\"The 109th bi-gram in the 70th essay is: {seventieth_essay_bigrams[108]}\")\n",
    "else:\n",
    "    print(f\"The 70th essay doesn't have 109 bi-grams. It only has {len(seventieth_essay_bigrams)} bi-grams.\")\n",
    "\n",
    "print(\"\\nFirst few bi-grams of the 70th essay:\")\n",
    "print(seventieth_essay_bigrams[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45f2f6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, we've conducted a comprehensive NLP analysis of student essays. We've explored various aspects of the text data, including:\n",
    "\n",
    "Basic text preprocessing and tokenization\n",
    "Word frequency analysis across all essays\n",
    "Unique word count and repetition patterns\n",
    "Personality type-specific word usage (focusing on ENFJ)\n",
    "Bi-gram creation and analysis\n",
    "\n",
    "These analyses provide insights into the language patterns used by students in their essays and how they might relate to personality types. Future work could involve more advanced NLP techniques such as sentiment analysis, topic modeling, or using machine learning to predict personality types based on essay content.\n",
    "This project demonstrates the power of NLP techniques in extracting meaningful information from unstructured text data, opening up possibilities for further research in areas like education, psychology, and linguistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
